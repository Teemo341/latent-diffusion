True
2
Global seed set to 23
Running on GPUs 0,1
LatentDiffusion: Running in eps-prediction mode
DiffusionWrapper has 295.02 M params.
Keeping EMAs of 522.
Restored from models/first_stage_models/HSI/VCA/model.ckpt
Training LatentDiffusion as an unconditional model.
Monitoring val/loss_simple_ema as checkpoint metric.
Merged modelckpt-cfg: 
{'target': 'pytorch_lightning.callbacks.ModelCheckpoint', 'params': {'dirpath': 'logs/2023-07-24T11-46-42_Indian_Pines_Corrected-ldm-VCA/checkpoints', 'filename': '{epoch:06}', 'verbose': True, 'save_last': True, 'monitor': 'val/loss_simple_ema', 'save_top_k': 3}}
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
Max training epoches: 1000, max training steps: None
#### Data #####
train, Indian_Pines_Corrected, 12996
accumulate_grad_batches = 1
Setting learning rate to 3.20e-03 = 1 (accumulate_grad_batches) * 2 (num_gpus) * 32 (batchsize) * 5.00e-05 (base_lr)
/home/home_node6_1/ssy1/anaconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:101: UserWarning: you defined a validation_step but have no val_dataloader. Skipping val loop
  rank_zero_warn(f"you defined a {step_name} but have no {loader_name}. Skipping {stage} loop")
Global seed set to 23
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/2
Global seed set to 23
Global seed set to 23
initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 2 processes
----------------------------------------------------------------------------------------------------

/home/home_node6_1/ssy1/anaconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.
  rank_zero_deprecation(
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [3,4]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [3,4]
Setting up LambdaLR scheduler...
Set SLURM handle signals.
Set SLURM handle signals.
Project config
Global seed set to 23
model:
  base_learning_rate: 5.0e-05
  target: ldm.models.diffusion.ddpm.LatentDiffusion
  params:
    linear_start: 0.0015
    linear_end: 0.0155
    num_timesteps_cond: 1
    log_every_t: 200
    timesteps: 1000
    loss_type: l1
    first_stage_key: image
    cond_stage_key: image
    image_size: 32
    channels: 20
    cond_stage_trainable: false
    concat_mode: false
    scale_by_std: true
    monitor: val/loss_simple_ema
    scheduler_config:
      target: ldm.lr_scheduler.LambdaLinearScheduler
      params:
        warm_up_steps:
        - 10000
        cycle_lengths:
        - 10000000000000
        f_start:
        - 1.0e-06
        f_max:
        - 1.0
        f_min:
        - 1.0
    unet_config:
      target: ldm.modules.diffusionmodules.openaimodel.UNetModel
      params:
        image_size: 32
        in_channels: 20
        out_channels: 20
        model_channels: 192
        attention_resolutions:
        - 1
        - 2
        - 4
        - 8
        num_res_blocks: 2
        channel_mult:
        - 1
        - 2
        - 2
        - 4
        - 4
        num_heads: 8
        use_scale_shift_norm: true
        resblock_updown: true
    first_stage_config:
      target: ldm.models.hsi_encoder.VCA
      params:
        in_channels: 200
        out_channels: 20
        ckpt_path: models/first_stage_models/HSI/VCA/model.ckpt
    cond_stage_config: __is_unconditional__
data:
  target: main_HSI.DataModuleFromConfig
  params:
    batch_size: 32
    num_workers: 5
    wrap: false
    train:
      target: ldm.data.HSI.Indian_Pines_Corrected
      params:
        size: 32

Lightning config
callbacks:
  image_logger:
    target: main_HSI.ImageLogger_HSI
    params:
      batch_frequency: 400
      max_images: 8
      increase_log_steps: false
      visual_channels:
      - 69
      - 27
      - 11
      log_images_kwargs:
        quantize_denoised: false
trainer:
  benchmark: true
  accelerator: ddp
  gpus: 0,1


  | Name              | Type             | Params
-------------------------------------------------------
0 | model             | DiffusionWrapper | 295 M 
1 | model_ema         | LitEma           | 0     
2 | first_stage_model | VCA              | 4.0 K 
-------------------------------------------------------
295 M     Trainable params
4.0 K     Non-trainable params
295 M     Total params
1,180.105 Total estimated model params size (MB)
Validation sanity check: 0it [00:00, ?it/s]                                           Global seed set to 23
Training: -1it [00:00, ?it/s]Training:   0%|          | 0/204 [00:00<00:00, 27235.74it/s]Epoch 0:   0%|          | 0/204 [00:00<00:00, 4588.95it/s]  True
2
Running on GPUs 0,1
LatentDiffusion: Running in eps-prediction mode
DiffusionWrapper has 295.02 M params.
Keeping EMAs of 522.
Restored from models/first_stage_models/HSI/VCA/model.ckpt
Training LatentDiffusion as an unconditional model.
Monitoring val/loss_simple_ema as checkpoint metric.
Merged modelckpt-cfg: 
{'target': 'pytorch_lightning.callbacks.ModelCheckpoint', 'params': {'dirpath': 'logs/2023-07-24T11-46-51_Indian_Pines_Corrected-ldm-VCA/checkpoints', 'filename': '{epoch:06}', 'verbose': True, 'save_last': True, 'monitor': 'val/loss_simple_ema', 'save_top_k': 3}}
Max training epoches: 1000, max training steps: None
#### Data #####
train, Indian_Pines_Corrected, 12996
accumulate_grad_batches = 1
Setting learning rate to 3.20e-03 = 1 (accumulate_grad_batches) * 2 (num_gpus) * 32 (batchsize) * 5.00e-05 (base_lr)
Setting up LambdaLR scheduler...
### USING STD-RESCALING ###
setting self.scale_factor to 1576.054443359375
### USING STD-RESCALING ###
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
