Start training Indian_Pines_Corrected dataset
warmup epoches 1/3
Epoch [1/30], Step [203/407], d_loss_real: 21.2943, d_loss_fake: 23.0279
Epoch [1/30], Step [406/407], d_loss_real: 21.3372, d_loss_fake: 22.9159
warmup epoches 2/3
Epoch [2/30], Step [203/407], d_loss_real: 21.4500, d_loss_fake: 22.5524
Epoch [2/30], Step [406/407], d_loss_real: 21.4831, d_loss_fake: 22.4077
warmup epoches 3/3
Epoch [3/30], Step [203/407], d_loss_real: 21.5850, d_loss_fake: 21.9412
Epoch [3/30], Step [406/407], d_loss_real: 21.6029, d_loss_fake: 21.7635
Epoch [4/30], Step [203/407], d_loss_real: 21.6482, d_loss_fake: 21.2701, g_loss: 23.1202
Epoch [4/30], Step [406/407], d_loss_real: 21.6495, d_loss_fake: 21.2283, g_loss: 23.1660
Epoch [5/30], Step [203/407], d_loss_real: 21.6652, d_loss_fake: 21.0320, g_loss: 23.3728
Epoch [5/30], Step [406/407], d_loss_real: 21.6466, d_loss_fake: 20.9200, g_loss: 23.4980
Epoch [6/30], Step [203/407], d_loss_real: 21.5932, d_loss_fake: 20.5675, g_loss: 23.8865
Epoch [6/30], Step [406/407], d_loss_real: 21.5879, d_loss_fake: 20.4138, g_loss: 24.0601
Epoch [7/30], Step [203/407], d_loss_real: 21.5269, d_loss_fake: 19.9470, g_loss: 24.5905
Epoch [7/30], Step [406/407], d_loss_real: 21.4880, d_loss_fake: 19.8242, g_loss: 24.7410
Epoch [8/30], Step [203/407], d_loss_real: 21.3731, d_loss_fake: 19.4533, g_loss: 25.1748
Epoch [8/30], Step [406/407], d_loss_real: 21.3486, d_loss_fake: 19.2892, g_loss: 25.3819
Epoch [9/30], Step [203/407], d_loss_real: 21.2376, d_loss_fake: 18.7043, g_loss: 26.0938
Epoch [9/30], Step [406/407], d_loss_real: 21.2010, d_loss_fake: 18.5245, g_loss: 26.3268
Epoch [10/30], Step [203/407], d_loss_real: 21.1292, d_loss_fake: 18.0318, g_loss: 26.9800
Epoch [10/30], Step [406/407], d_loss_real: 21.0923, d_loss_fake: 17.8889, g_loss: 27.1669
Epoch [11/30], Step [203/407], d_loss_real: 21.0207, d_loss_fake: 17.8379, g_loss: 27.2464
Epoch [11/30], Step [406/407], d_loss_real: 20.9675, d_loss_fake: 17.6695, g_loss: 27.4682
Epoch [12/30], Step [203/407], d_loss_real: 20.7776, d_loss_fake: 17.2620, g_loss: 28.0341
Epoch [12/30], Step [406/407], d_loss_real: 20.7496, d_loss_fake: 17.0412, g_loss: 28.3780
Epoch [13/30], Step [203/407], d_loss_real: 20.6046, d_loss_fake: 16.0812, g_loss: 29.7630
Epoch [13/30], Step [406/407], d_loss_real: 20.5581, d_loss_fake: 16.2899, g_loss: 29.4505
Epoch [14/30], Step [203/407], d_loss_real: 20.3934, d_loss_fake: 16.1896, g_loss: 29.6208
Epoch [14/30], Step [406/407], d_loss_real: 20.3163, d_loss_fake: 15.9278, g_loss: 30.0171
Epoch [15/30], Step [203/407], d_loss_real: 20.1272, d_loss_fake: 15.6516, g_loss: 30.4745
Epoch [15/30], Step [406/407], d_loss_real: 20.1125, d_loss_fake: 15.4155, g_loss: 30.8759
Epoch [16/30], Step [203/407], d_loss_real: 20.6692, d_loss_fake: 17.7862, g_loss: 27.4389
Epoch [16/30], Step [406/407], d_loss_real: 21.1585, d_loss_fake: 17.3181, g_loss: 28.0893
Epoch [17/30], Step [203/407], d_loss_real: 21.6387, d_loss_fake: 16.5369, g_loss: 29.1554
Epoch [17/30], Step [406/407], d_loss_real: 21.5046, d_loss_fake: 16.6811, g_loss: 28.9465
Epoch [18/30], Step [203/407], d_loss_real: 20.9971, d_loss_fake: 15.4452, g_loss: 30.8449
Epoch [18/30], Step [406/407], d_loss_real: 20.7612, d_loss_fake: 15.0577, g_loss: 31.4854
Epoch [19/30], Step [203/407], d_loss_real: 20.2251, d_loss_fake: 14.0475, g_loss: 33.1664
Epoch [19/30], Step [406/407], d_loss_real: 20.0417, d_loss_fake: 14.0096, g_loss: 33.2277
Epoch [20/30], Step [203/407], d_loss_real: 19.7529, d_loss_fake: 15.7340, g_loss: 30.5952
Epoch [20/30], Step [406/407], d_loss_real: 20.1011, d_loss_fake: 17.5199, g_loss: 28.0794
Epoch [21/30], Step [203/407], d_loss_real: 21.4290, d_loss_fake: 21.8632, g_loss: 22.5704
Epoch [21/30], Step [406/407], d_loss_real: 22.0236, d_loss_fake: 20.7101, g_loss: 23.8676
Epoch [22/30], Step [203/407], d_loss_real: 23.4775, d_loss_fake: 18.0636, g_loss: 27.0442
Epoch [22/30], Step [406/407], d_loss_real: 23.9468, d_loss_fake: 17.6973, g_loss: 27.5264
Epoch [23/30], Step [203/407], d_loss_real: 24.0947, d_loss_fake: 16.7458, g_loss: 28.8602
Epoch [23/30], Step [406/407], d_loss_real: 24.2529, d_loss_fake: 16.9658, g_loss: 28.5731
Epoch [24/30], Step [203/407], d_loss_real: 23.4540, d_loss_fake: 16.4661, g_loss: 29.3198
Epoch [24/30], Step [406/407], d_loss_real: 23.1156, d_loss_fake: 16.0351, g_loss: 29.9787
Epoch [25/30], Step [203/407], d_loss_real: 21.3679, d_loss_fake: 14.5855, g_loss: 32.2431
Epoch [25/30], Step [406/407], d_loss_real: 20.9741, d_loss_fake: 14.9602, g_loss: 31.6101
Epoch [26/30], Step [203/407], d_loss_real: 21.1925, d_loss_fake: 15.2604, g_loss: 31.1627
Epoch [26/30], Step [406/407], d_loss_real: 21.0096, d_loss_fake: 16.0362, g_loss: 29.9873
Epoch [27/30], Step [203/407], d_loss_real: 19.9422, d_loss_fake: 16.1351, g_loss: 29.7781
Epoch [27/30], Step [406/407], d_loss_real: 19.7012, d_loss_fake: 16.5427, g_loss: 29.1682
Epoch [28/30], Step [203/407], d_loss_real: 19.6197, d_loss_fake: 17.0409, g_loss: 28.4705
Epoch [28/30], Step [406/407], d_loss_real: 19.8263, d_loss_fake: 16.7756, g_loss: 28.8983
Epoch [29/30], Step [203/407], d_loss_real: 20.9663, d_loss_fake: 15.5867, g_loss: 30.7385
Epoch [29/30], Step [406/407], d_loss_real: 20.7893, d_loss_fake: 15.4241, g_loss: 30.9563
Epoch [30/30], Step [203/407], d_loss_real: 19.8551, d_loss_fake: 14.1828, g_loss: 32.9831
Epoch [30/30], Step [406/407], d_loss_real: 19.4898, d_loss_fake: 14.0586, g_loss: 33.1977
Finish training Indian_Pines_Corrected dataset
Save checkpoint to ./experiments/models/checkpoints/GAN/Indian_Pines_Corrected
finish all datasets
