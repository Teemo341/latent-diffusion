True
3
Global seed set to 3407
Running on GPUs 0,1,2
LatentDiffusion: Running in eps-prediction mode
DiffusionWrapper has 295.01 M params.
Keeping EMAs of 522.
/data_ssy/latent-diffusion/ldm/models/hsi_encoder.py:186: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(path, map_location="cpu")["state_dict"]
Restored from models/first_stage_models/HSI/VCA/Indian_Pines_Corrected/model.ckpt
Training LatentDiffusion as an unconditional model.
Monitoring val/loss_simple_ema as checkpoint metric.
Merged modelckpt-cfg: 
{'target': 'pytorch_lightning.callbacks.ModelCheckpoint', 'params': {'dirpath': 'logs/2024-11-10T18-44-40_Indian_Pines_Corrected-ldm-VCA/checkpoints', 'filename': '{epoch:06}', 'verbose': True, 'save_last': True, 'monitor': 'val/loss_simple_ema', 'save_top_k': 3}}
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
Max training epoches: 1, max training steps: None
#### Data #####
train, Indian_Pines_Corrected_train, 10396
validation, Indian_Pines_Corrected_valid, 2600
accumulate_grad_batches = 1
Setting learning rate to 9.60e-04 = 1 (accumulate_grad_batches) * 3 (num_gpus) * 32 (batchsize) * 1.00e-05 (base_lr)
Global seed set to 3407
Global seed set to 3407
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/3
Global seed set to 3407
/data_ssy/latent-diffusion/ldm/models/hsi_encoder.py:186: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(path, map_location="cpu")["state_dict"]
Global seed set to 3407
initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/3
/data_ssy/latent-diffusion/ldm/models/hsi_encoder.py:186: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(path, map_location="cpu")["state_dict"]
Global seed set to 3407
initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/3
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 3 processes
----------------------------------------------------------------------------------------------------

/home/home_node6_1/ssy1/anaconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.
  rank_zero_deprecation(
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [1,2,3]
Setting up LambdaLR scheduler...
Set SLURM handle signals.
Set SLURM handle signals.
Set SLURM handle signals.
Project config
model:
  base_learning_rate: 1.0e-05
  target: ldm.models.diffusion.ddpm_HSI.LatentDiffusion
  params:
    linear_start: 0.0015
    linear_end: 0.0155
    num_timesteps_cond: 1
    log_every_t: 200
    timesteps: 1000
    loss_type: l2
    first_stage_key: image
    cond_stage_key: image
    image_size: 32
    channels: 16
    cond_stage_trainable: false
    concat_mode: false
    scale_by_std: false
    scale_factor: 0.5
    monitor: val/loss_simple_ema
    scheduler_config:
      target: ldm.lr_scheduler.LambdaLinearScheduler
      params:
        warm_up_steps:
        - 10000
        cycle_lengths:
        - 10000000000000
        f_start:
        - 1.0e-06
        f_max:
        - 1.0
        f_min:
        - 1.0
    unet_config:
      target: ldm.modules.diffusionmodules.openaimodel.UNetModel
      params:
        image_size: 32
        in_channels: 16
        out_channels: 16
        model_channels: 192
        attention_resolutions:
        - 1
        - 2
        - 4
        - 8
        num_res_blocks: 2
        channel_mult:
        - 1
        - 2
        - 2
        - 4
        - 4
        num_heads: 8
        use_scale_shift_norm: true
        resblock_updown: true
    first_stage_config:
      target: ldm.models.hsi_encoder.VCA
      params:
        in_channels: 200
        out_channels: 16
        ckpt_path: models/first_stage_models/HSI/VCA/Indian_Pines_Corrected/model.ckpt
    cond_stage_config: __is_unconditional__
data:
  target: main_HSI.DataModuleFromConfig
  params:
    batch_size: 32
    num_workers: 5
    wrap: false
    train:
      target: ldm.data.HSI.Indian_Pines_Corrected_train
      params:
        size: 32
    validation:
      target: ldm.data.HSI.Indian_Pines_Corrected_valid
      params:
        size: 32

Lightning config
callbacks:
  image_logger:
    target: main_HSI.ImageLogger_HSI
    params:
      batch_frequency: 100
      max_images: 8
      increase_log_steps: false
      visual_channels:
      - 36
      - 17
      - 11
      log_images_kwargs:
        quantize_denoised: false
        inpaint: false
        plot_denoise_rows: false
        plot_progressive_rows: true
        plot_diffusion_rows: true
trainer:
  benchmark: true
  max_epochs: 1
  accelerator: ddp
  gpus: 0,1,2


  | Name              | Type             | Params
-------------------------------------------------------
0 | model             | DiffusionWrapper | 295 M 
1 | model_ema         | LitEma           | 0     
2 | first_stage_model | VCA              | 3.2 K 
-------------------------------------------------------
295 M     Trainable params
3.2 K     Non-trainable params
295 M     Total params
1,180.046 Total estimated model params size (MB)
Validation sanity check: 0it [00:00, ?it/s]Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]True
3
Running on GPUs 0,1,2
LatentDiffusion: Running in eps-prediction mode
DiffusionWrapper has 295.01 M params.
Keeping EMAs of 522.
Restored from models/first_stage_models/HSI/VCA/Indian_Pines_Corrected/model.ckpt
Training LatentDiffusion as an unconditional model.
Monitoring val/loss_simple_ema as checkpoint metric.
Merged modelckpt-cfg: 
{'target': 'pytorch_lightning.callbacks.ModelCheckpoint', 'params': {'dirpath': 'logs/2024-11-10T18-44-51_Indian_Pines_Corrected-ldm-VCA/checkpoints', 'filename': '{epoch:06}', 'verbose': True, 'save_last': True, 'monitor': 'val/loss_simple_ema', 'save_top_k': 3}}
Max training epoches: 1, max training steps: None
#### Data #####
train, Indian_Pines_Corrected_train, 10396
validation, Indian_Pines_Corrected_valid, 2600
accumulate_grad_batches = 1
Setting learning rate to 9.60e-04 = 1 (accumulate_grad_batches) * 3 (num_gpus) * 32 (batchsize) * 1.00e-05 (base_lr)
Setting up LambdaLR scheduler...
True
3
Running on GPUs 0,1,2
LatentDiffusion: Running in eps-prediction mode
DiffusionWrapper has 295.01 M params.
Keeping EMAs of 522.
Restored from models/first_stage_models/HSI/VCA/Indian_Pines_Corrected/model.ckpt
Training LatentDiffusion as an unconditional model.
Monitoring val/loss_simple_ema as checkpoint metric.
Merged modelckpt-cfg: 
{'target': 'pytorch_lightning.callbacks.ModelCheckpoint', 'params': {'dirpath': 'logs/2024-11-10T18-44-49_Indian_Pines_Corrected-ldm-VCA/checkpoints', 'filename': '{epoch:06}', 'verbose': True, 'save_last': True, 'monitor': 'val/loss_simple_ema', 'save_top_k': 3}}
Max training epoches: 1, max training steps: None
#### Data #####
train, Indian_Pines_Corrected_train, 10396
validation, Indian_Pines_Corrected_valid, 2600
accumulate_grad_batches = 1
Setting learning rate to 9.60e-04 = 1 (accumulate_grad_batches) * 3 (num_gpus) * 32 (batchsize) * 1.00e-05 (base_lr)
Setting up LambdaLR scheduler...
Validation sanity check:  50%|█████     | 1/2 [00:05<00:05,  5.13s/it]Validation sanity check: 100%|██████████| 2/2 [00:05<00:00,  2.42s/it]                                                                      Global seed set to 3407
Global seed set to 3407
Global seed set to 3407
Training: -1it [00:00, ?it/s]Training:   0%|          | 0/137 [00:00<00:00, 25731.93it/s]Epoch 0:   0%|          | 0/137 [00:00<00:00, 4573.94it/s]  [rank1]:[W1110 18:45:06.333011271 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank2]:[W1110 18:45:06.356037810 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank0]:[W1110 18:45:06.466181688 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
Epoch 0:   1%|          | 1/137 [00:11<13:15,  5.85s/it]  Epoch 0:   1%|          | 1/137 [00:11<13:15,  5.85s/it, loss=1, v_num=0, train/loss_simple_step=1.000, train/loss_vlb_step=0.0082, train/loss_step=1.000, global_step=0.000, lr_abs=9.6e-10]Epoch 0:   1%|▏         | 2/137 [00:13<09:45,  4.34s/it, loss=1, v_num=0, train/loss_simple_step=1.000, train/loss_vlb_step=0.0082, train/loss_step=1.000, global_step=0.000, lr_abs=9.6e-10]Epoch 0:   1%|▏         | 2/137 [00:13<09:45,  4.34s/it, loss=1, v_num=0, train/loss_simple_step=1.000, train/loss_vlb_step=0.00698, train/loss_step=1.000, global_step=1.000, lr_abs=9.7e-8]Epoch 0:   2%|▏         | 3/137 [00:14<08:00,  3.58s/it, loss=1, v_num=0, train/loss_simple_step=1.000, train/loss_vlb_step=0.00698, train/loss_step=1.000, global_step=1.000, lr_abs=9.7e-8]Epoch 0:   2%|▏         | 3/137 [00:14<08:00,  3.58s/it, loss=1, v_num=0, train/loss_simple_step=1.000, train/loss_vlb_step=0.00505, train/loss_step=1.000, global_step=2.000, lr_abs=1.93e-7]Epoch 0:   3%|▎         | 4/137 [00:15<06:55,  3.13s/it, loss=1, v_num=0, train/loss_simple_step=1.000, train/loss_vlb_step=0.00505, train/loss_step=1.000, global_step=2.000, lr_abs=1.93e-7]Epoch 0:   3%|▎         | 4/137 [00:15<06:55,  3.13s/it, loss=1, v_num=0, train/loss_simple_step=0.999, train/loss_vlb_step=0.00696, train/loss_step=0.999, global_step=3.000, lr_abs=2.89e-7]Epoch 0:   4%|▎         | 5/137 [00:16<06:12,  2.83s/it, loss=1, v_num=0, train/loss_simple_step=0.999, train/loss_vlb_step=0.00696, train/loss_step=0.999, global_step=3.000, lr_abs=2.89e-7]Epoch 0:   4%|▎         | 5/137 [00:16<06:13,  2.83s/it, loss=1, v_num=0, train/loss_simple_step=0.998, train/loss_vlb_step=0.014, train/loss_step=0.998, global_step=4.000, lr_abs=3.85e-7]  Epoch 0:   4%|▍         | 6/137 [00:18<05:41,  2.61s/it, loss=1, v_num=0, train/loss_simple_step=0.998, train/loss_vlb_step=0.014, train/loss_step=0.998, global_step=4.000, lr_abs=3.85e-7]Epoch 0:   4%|▍         | 6/137 [00:18<05:41,  2.61s/it, loss=1, v_num=0, train/loss_simple_step=1.000, train/loss_vlb_step=0.00513, train/loss_step=1.000, global_step=5.000, lr_abs=4.81e-7]Epoch 0:   5%|▌         | 7/137 [00:19<05:18,  2.45s/it, loss=1, v_num=0, train/loss_simple_step=1.000, train/loss_vlb_step=0.00513, train/loss_step=1.000, global_step=5.000, lr_abs=4.81e-7]Epoch 0:   5%|▌         | 7/137 [00:19<05:18,  2.45s/it, loss=1, v_num=0, train/loss_simple_step=1.000, train/loss_vlb_step=0.00516, train/loss_step=1.000, global_step=6.000, lr_abs=5.77e-7]Epoch 0:   6%|▌         | 8/137 [00:20<04:59,  2.32s/it, loss=1, v_num=0, train/loss_simple_step=1.000, train/loss_vlb_step=0.00516, train/loss_step=1.000, global_step=6.000, lr_abs=5.77e-7]Epoch 0:   6%|▌         | 8/137 [00:20<04:59,  2.32s/it, loss=1, v_num=0, train/loss_simple_step=0.998, train/loss_vlb_step=0.0051, train/loss_step=0.998, global_step=7.000, lr_abs=6.73e-7] Epoch 0:   7%|▋         | 9/137 [00:22<04:44,  2.22s/it, loss=1, v_num=0, train/loss_simple_step=0.998, train/loss_vlb_step=0.0051, train/loss_step=0.998, global_step=7.000, lr_abs=6.73e-7]Epoch 0:   7%|▋         | 9/137 [00:22<04:44,  2.22s/it, loss=1, v_num=0, train/loss_simple_step=0.997, train/loss_vlb_step=0.00713, train/loss_step=0.997, global_step=8.000, lr_abs=7.69e-7]Epoch 0:   7%|▋         | 10/137 [00:23<04:31,  2.14s/it, loss=1, v_num=0, train/loss_simple_step=0.997, train/loss_vlb_step=0.00713, train/loss_step=0.997, global_step=8.000, lr_abs=7.69e-7]Epoch 0:   7%|▋         | 10/137 [00:23<04:31,  2.14s/it, loss=0.999, v_num=0, train/loss_simple_step=0.997, train/loss_vlb_step=0.00515, train/loss_step=0.997, global_step=9.000, lr_abs=8.65e-7]Epoch 0:   8%|▊         | 11/137 [00:24<04:21,  2.07s/it, loss=0.999, v_num=0, train/loss_simple_step=0.997, train/loss_vlb_step=0.00515, train/loss_step=0.997, global_step=9.000, lr_abs=8.65e-7]Epoch 0:   8%|▊         | 11/137 [00:24<04:21,  2.07s/it, loss=0.999, v_num=0, train/loss_simple_step=1.000, train/loss_vlb_step=0.00592, train/loss_step=1.000, global_step=10.00, lr_abs=9.61e-7]Epoch 0:   9%|▉         | 12/137 [00:26<04:11,  2.01s/it, loss=0.999, v_num=0, train/loss_simple_step=1.000, train/loss_vlb_step=0.00592, train/loss_step=1.000, global_step=10.00, lr_abs=9.61e-7]Epoch 0:   9%|▉         | 12/137 [00:26<04:11,  2.01s/it, loss=0.999, v_num=0, train/loss_simple_step=0.996, train/loss_vlb_step=0.0215, train/loss_step=0.996, global_step=11.00, lr_abs=1.06e-6] Epoch 0:   9%|▉         | 13/137 [00:27<04:03,  1.96s/it, loss=0.999, v_num=0, train/loss_simple_step=0.996, train/loss_vlb_step=0.0215, train/loss_step=0.996, global_step=11.00, lr_abs=1.06e-6]Epoch 0:   9%|▉         | 13/137 [00:27<04:03,  1.96s/it, loss=0.999, v_num=0, train/loss_simple_step=1.000, train/loss_vlb_step=0.00562, train/loss_step=1.000, global_step=12.00, lr_abs=1.15e-6]Epoch 0:  10%|█         | 14/137 [00:28<03:56,  1.92s/it, loss=0.999, v_num=0, train/loss_simple_step=1.000, train/loss_vlb_step=0.00562, train/loss_step=1.000, global_step=12.00, lr_abs=1.15e-6]Epoch 0:  10%|█         | 14/137 [00:28<03:56,  1.92s/it, loss=0.999, v_num=0, train/loss_simple_step=1.000, train/loss_vlb_step=0.00695, train/loss_step=1.000, global_step=13.00, lr_abs=1.25e-6]Epoch 0:  11%|█         | 15/137 [00:30<03:49,  1.88s/it, loss=0.999, v_num=0, train/loss_simple_step=1.000, train/loss_vlb_step=0.00695, train/loss_step=1.000, global_step=13.00, lr_abs=1.25e-6]Epoch 0:  11%|█         | 15/137 [00:30<03:49,  1.88s/it, loss=1, v_num=0, train/loss_simple_step=1.000, train/loss_vlb_step=0.0247, train/loss_step=1.000, global_step=14.00, lr_abs=1.34e-6]     Epoch 0:  12%|█▏        | 16/137 [00:31<03:43,  1.85s/it, loss=1, v_num=0, train/loss_simple_step=1.000, train/loss_vlb_step=0.0247, train/loss_step=1.000, global_step=14.00, lr_abs=1.34e-6]Epoch 0:  12%|█▏        | 16/137 [00:31<03:43,  1.85s/it, loss=0.999, v_num=0, train/loss_simple_step=0.996, train/loss_vlb_step=0.00572, train/loss_step=0.996, global_step=15.00, lr_abs=1.44e-6]Epoch 0:  12%|█▏        | 17/137 [00:32<03:38,  1.82s/it, loss=0.999, v_num=0, train/loss_simple_step=0.996, train/loss_vlb_step=0.00572, train/loss_step=0.996, global_step=15.00, lr_abs=1.44e-6]Epoch 0:  12%|█▏        | 17/137 [00:32<03:38,  1.82s/it, loss=0.999, v_num=0, train/loss_simple_step=0.997, train/loss_vlb_step=0.00495, train/loss_step=0.997, global_step=16.00, lr_abs=1.54e-6]Epoch 0:  13%|█▎        | 18/137 [00:34<03:33,  1.79s/it, loss=0.999, v_num=0, train/loss_simple_step=0.997, train/loss_vlb_step=0.00495, train/loss_step=0.997, global_step=16.00, lr_abs=1.54e-6]Epoch 0:  13%|█▎        | 18/137 [00:34<03:33,  1.79s/it, loss=0.999, v_num=0, train/loss_simple_step=0.997, train/loss_vlb_step=0.00627, train/loss_step=0.997, global_step=17.00, lr_abs=1.63e-6]